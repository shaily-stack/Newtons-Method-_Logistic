{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38232bit3f3267c9903a4bc98b330e5110826879",
   "display_name": "Python 3.8.2 32-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "X, y= make_classification(n_samples= 500, n_features= 4, random_state=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "SyntaxError",
     "evalue": "positional argument follows keyword argument (<ipython-input-22-9e6fe487eadd>, line 1)",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-22-9e6fe487eadd>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    data = pd.DataFrame(data = X, y)\u001b[0m\n\u001b[1;37m                                  ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m positional argument follows keyword argument\n"
     ]
    }
   ],
   "source": [
    "data = pd.DataFrame(data = X) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['target'] = y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "            0         1         2         3  target\n0    1.180984 -0.392328  0.395615  1.490950       1\n1    1.038899 -0.353167  0.407637  1.252005       1\n2   -0.167539 -0.221197  1.996933 -2.262845       1\n3    0.185295 -0.277285  1.661847 -1.364504       1\n4   -2.188238  0.636068 -0.059133 -3.435906       1\n..        ...       ...       ...       ...     ...\n495  0.891653 -0.553922  2.209786 -0.783809       1\n496  0.738983 -0.467830  1.896323 -0.714449       1\n497  1.394085 -0.647912  1.837344  0.390790       1\n498  0.206085  0.023928 -0.616098  0.944735       0\n499 -0.253475 -0.086339  1.179789 -1.583643       1\n\n[500 rows x 5 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1.180984</td>\n      <td>-0.392328</td>\n      <td>0.395615</td>\n      <td>1.490950</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1.038899</td>\n      <td>-0.353167</td>\n      <td>0.407637</td>\n      <td>1.252005</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-0.167539</td>\n      <td>-0.221197</td>\n      <td>1.996933</td>\n      <td>-2.262845</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.185295</td>\n      <td>-0.277285</td>\n      <td>1.661847</td>\n      <td>-1.364504</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>-2.188238</td>\n      <td>0.636068</td>\n      <td>-0.059133</td>\n      <td>-3.435906</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>495</th>\n      <td>0.891653</td>\n      <td>-0.553922</td>\n      <td>2.209786</td>\n      <td>-0.783809</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>496</th>\n      <td>0.738983</td>\n      <td>-0.467830</td>\n      <td>1.896323</td>\n      <td>-0.714449</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>497</th>\n      <td>1.394085</td>\n      <td>-0.647912</td>\n      <td>1.837344</td>\n      <td>0.390790</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>498</th>\n      <td>0.206085</td>\n      <td>0.023928</td>\n      <td>-0.616098</td>\n      <td>0.944735</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>499</th>\n      <td>-0.253475</td>\n      <td>-0.086339</td>\n      <td>1.179789</td>\n      <td>-1.583643</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>500 rows Ã— 5 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Using Newton's Method for logistic regression optimisation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(b0, b1, X):\n",
    "    return 1/(1+np.exp(-(b0 + b1 *X)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_likelihood(b0, b1, x, y):\n",
    "    sigmoid_prob = sigmoid(b0, b1, x)\n",
    "    return np.sum(y*np.log(sigmoid_prob) + (1-y)*np.log(1- sigmoid_prob) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jacobian(b0, b1, x, y):\n",
    "    sigmoid_prob = sigmoid(b0, b1, x) \n",
    "    return np.array([[np.sum((y - sigmoid_prob) * x),\n",
    "                    np.sum(y - sigmoid_prob)]])\n",
    "\n",
    "\n",
    "def hessian(b0, b1, x, y):\n",
    "    sigmoid_prob = sigmoid(b0, b1, x)\n",
    "    a11 = np.sum(sigmoid_prob * (1 - sigmoid_prob) * x * x)\n",
    "    a12 = np.sum(sigmoid_prob * (1 - sigmoid_prob) * x * 1)\n",
    "    a22 = np.sum(sigmoid_prob * (1 - sigmoid_prob) * 1 * 1)\n",
    "    return np.array([[a11, a12],[a12, a22]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def newtons_method(x, y):\n",
    "    \"\"\"\n",
    "    :input data x(np.array(float))\n",
    "    :outplut data y(np.array(bool))\n",
    "    :returns np.array of logreg's parameters after convergence to global maxima for strictly concave function( because of taking log of likelihood)\n",
    "    \"\"\"\n",
    "\n",
    "    #Initialising log_likelihood and parameters b0, b1\n",
    "    b0 = 0\n",
    "    b1 = 2\n",
    "    cost_reduction = np.Infinity\n",
    "    l = log_likelihood(b0, b1, x, y)\n",
    "\n",
    "    #Convergence condition\n",
    "    delta = 0.0000000001\n",
    "    max_iterations = 15\n",
    "    \n",
    "    i = 0\n",
    "\n",
    "    while abs(cost_reduction) > delta and i < max_iterations:\n",
    "        i +=1\n",
    "        g = jacobian(b0, b1, x, y)\n",
    "        h = hessian(b0, b1,x, y) \n",
    "        h_inv = np.linalg.inv(h)\n",
    "\n",
    "        update = h_inv @ g.T\n",
    "        update_b0 = update[0][0]\n",
    "        update_b1 = update[1][0]\n",
    "\n",
    "        # Perform uodate steps\n",
    "        b0 += update_b0\n",
    "        b1 += update_b1\n",
    "\n",
    "        #update the loglikelihood for each iteration\n",
    "        l_new = log_likelihood(b0, b1, x, y)\n",
    "        cost_reduction = l - l_new\n",
    "        l = l_new\n",
    "\n",
    "    return np.array([b0, b1])        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([ 60973502.87043228, -52670840.50103921])"
     },
     "metadata": {},
     "execution_count": 40
    }
   ],
   "source": [
    "x = data[0]\n",
    "y = data['target']\n",
    "\n",
    "newtons_method(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([2.38566146e+11, 2.04219272e+11])"
     },
     "metadata": {},
     "execution_count": 46
    }
   ],
   "source": [
    "x = data[0]\n",
    "y = data['target']\n",
    "\n",
    "newtons_method(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([-4.90698418e+25,  4.66373099e+25])"
     },
     "metadata": {},
     "execution_count": 48
    }
   ],
   "source": [
    "x = data[0]\n",
    "y = data['target']\n",
    "\n",
    "newtons_method(x, y)"
   ]
  },
  {
   "source": [
    "Resources\n",
    "\n",
    "\n",
    "https://thelaziestprogrammer.com/sharrington/math-of-machine-learning/solving-logreg-newtons-method#:~:text=Logistic%20Regression%20introduces%20the%20concept,transformation%20called%20the%20sigmoid%20function.\n",
    "\n",
    "\n",
    "\n",
    "https://medium.com/@papillonbee/logistic-regression-from-scratch-with-gradient-descent-and-newtons-method-ff4307e3cb30\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}